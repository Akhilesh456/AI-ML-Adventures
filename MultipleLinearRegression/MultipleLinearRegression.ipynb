{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data, droping unimportant deatures, feature scalling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"kc_house_data.csv\")\n",
    "Y_train = train_data['price'].to_numpy()/1000000\n",
    "# droping the unnecessarry features\n",
    "train_data = train_data.drop(['id', 'date', 'zipcode', 'lat', 'long', 'price'], axis=1)\n",
    "rows = len(train_data.axes[0])\n",
    "cols = len(train_data.axes[1])\n",
    "n = cols\n",
    "m = rows\n",
    "\n",
    "# initiallizing the np array\n",
    "X_train = np.zeros((rows, cols))\n",
    "\n",
    "# Feature Scalling\n",
    "for i in range(cols):\n",
    "    cmin = train_data.iloc[:, i].min()\n",
    "    cmax = train_data.iloc[:, i].max()\n",
    "    col_mean = train_data.iloc[:, i].sum()/m\n",
    "    X_train[:,i] = (train_data.iloc[:, i]-col_mean)/(cmax-cmin)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, Y, w, b):\n",
    "    \"\"\"Calculates the cost function\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Input data\n",
    "        Y (np.array): Correct output data\n",
    "        w (np.array): w vector\n",
    "        b (Number): b parameter\n",
    "\n",
    "    Returns:\n",
    "        Number: Returns cost\n",
    "    \"\"\"\n",
    "    s = 0\n",
    "    for i in range(m):\n",
    "        f_wb = np.dot(w, X[i]) + b\n",
    "        s += (f_wb - Y[i])**2\n",
    "    cost = s/(2*m)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def compute_gradient(X, Y, w, b):\n",
    "    \"\"\"Computes the gradient of cost function at w and b\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Training inputs\n",
    "        y (np.array): Training oupts (Correct answers)\n",
    "        w (np.array): w vector\n",
    "        b (Number): b parameter\n",
    "\n",
    "    Returns:\n",
    "        np.array, Number: dj_dw and dj_db\n",
    "    \"\"\"   \n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0\n",
    "    for j in range(n):\n",
    "        t = 0\n",
    "        for i in range(m):\n",
    "            f_wb = np.dot(w, X[i]) + b\n",
    "            er = f_wb - Y[i]\n",
    "            t += er*X[i][j]\n",
    "            if(j == 0):\n",
    "                dj_db += er\n",
    "        dj_dw[j] = t/m\n",
    "    dj_db = dj_db/m\n",
    "          \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def GD_algorithm(X, Y, alpha):\n",
    "    \"\"\"Calculates the best fit for the model f_w,b(X) = w*X + b for the given data and learning rate\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Inputs of training data\n",
    "        y (np.array): Correct key of output data\n",
    "        alpha (Number): Learning Rate\n",
    "\n",
    "    Returns:\n",
    "        w, b, J_history: Parameters and Cost function history for Learning Curve\n",
    "    \"\"\"   \n",
    "    J_history = []\n",
    "    w = np.zeros(n)\n",
    "    b = 0\n",
    "    prev_J = cost_function(X, Y, w, b)\n",
    "    iterations = 0\n",
    "    while(True):\n",
    "        iterations += 1\n",
    "        dj_dw, dj_db = compute_gradient(X, Y, w, b)\n",
    "        w = w - alpha*dj_dw\n",
    "        b = b - alpha*dj_db\n",
    "        curr_J = cost_function(X, Y, w, b)\n",
    "        \n",
    "        J_history.append(curr_J)\n",
    "        # print(f\"J: {curr_J}\")\n",
    "        if(curr_J > prev_J):\n",
    "            print(f\"The value of alpha: {alpha} is too large, try a smaller value.\")\n",
    "            break\n",
    "        if(prev_J-curr_J < 0.0001):\n",
    "            return w, b, J_history, iterations\n",
    "        \n",
    "        prev_J = curr_J\n",
    "        \n",
    "def predict(X, w, b):\n",
    "    \"\"\"Predicts the data based on the input parameters\n",
    "\n",
    "    Args:\n",
    "        x (np.array): Inputs\n",
    "        w (np.array): w vector\n",
    "        b (Number): b parameter\n",
    "\n",
    "    Returns:\n",
    "        np.array: Predictions\n",
    "    \"\"\"\n",
    "    Y_pred = []   \n",
    "    m = len(X)\n",
    "    \n",
    "    for i in range(m):\n",
    "        Y_pred.append(np.dot(w, X[i]) + b)\n",
    "        \n",
    "    return Y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "alpha = 0.1\n",
    "w, b, J_history, iterations = GD_algorithm(X_train, Y_train, alpha)\n",
    "\n",
    "# Plotting Learning Curve\n",
    "s = len(J_history)\n",
    "indices = []\n",
    "for i in range(1, s+1):\n",
    "    indices.append(i+iterations-s)\n",
    "plt.plot(indices, J_history, c='b')\n",
    "plt.xlabel(\"# Iterations\")\n",
    "plt.ylabel(\"Cost Function\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70cee1ab31432a3c6d7a7399fb312c5ef240bc8620d15cade7df88066d7533ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
